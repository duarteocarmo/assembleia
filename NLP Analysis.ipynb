{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analise preliminar de linguagem nos discursos do Parlamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Importar dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Começamos por importar algumas libraries necessárias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "\n",
    "from spacy.lang.pt import Portuguese\n",
    "from nltk.stem import RSLPStemmer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E extraimos um numero limitado de discursos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_FILES_TO_LOAD = 30\n",
    "FILES_DIRECTORY = \"data/debates/\"\n",
    "\n",
    "parties = []\n",
    "speakers = []\n",
    "contents = []\n",
    "\n",
    "for file_name in os.listdir(FILES_DIRECTORY)[:JSON_FILES_TO_LOAD]:\n",
    "    with open(FILES_DIRECTORY + file_name) as file:\n",
    "        json_data = json.load(file)\n",
    "        for number, intervention in json_data['intervenções'].items():\n",
    "            speakers.append(intervention[\"orador\"])\n",
    "            parties.append(intervention[\"partido\"])\n",
    "            contents.append(intervention[\"discurso\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos visualizar um exemplo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orador:  Cristóvão Simão Ribeiro\n",
      "Partido:  PSD\n",
      "Discurso:  Sr. Presidente, Srs. Membros do Governo, Sr.as Deputadas e Srs. Deputados: Dirijo um cumprimento especial aos estudantes de medicina que se encontram a assistir à sessão. Vou citar um ex-Primeiro-Ministro e começar por dizer o seguinte: quando queremos ser coerentes não podemos ser originais todos os dias. Este é um assunto de tal maneira debatido e replicado nesta Câmara que vou dizer exatamente aquilo que disse nas últimas três ou quatro vezes que este assunto foi aqui trazido repetidamente desde que os senhores são Governo e que a esquerda parlamentar os apoia. Vou dizê-lo em quatro notas particulares. Primeira nota: é evidentemente lesivo e um autêntico defraudar de expetativas, quer para os estudantes, quer para as suas famílias e, ainda pior, altamente lesivo para o Serviço Nacional de Saúde aquilo que é o desperdício de rios de dinheiro em formação médica sem depois haver uma consequência na formação médica especializada que, justamente, esses estudantes e esses jovens ambicionam e que legitimamente os utentes do SNS precisam.\n"
     ]
    }
   ],
   "source": [
    "indice = 899\n",
    "print(\"Orador: \", speakers[indice])\n",
    "print(\"Partido: \", parties[indice])\n",
    "print(\"Discurso: \", contents[indice])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantos recolhemos no total? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oradores: 364\n",
      "Partidos: 8\n",
      "Discursos: 6988\n"
     ]
    }
   ],
   "source": [
    "print(f\"Oradores: {len(list(set(speakers)))}\")\n",
    "print(f\"Partidos: {len(list(set(parties)))}\")\n",
    "print(f\"Discursos: {len(contents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Limpeza de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **De palavras para tokens limpos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criamos a função `tokenize`,  uma função que limpa o texto de acordo com um numero de regras (é texto? é um numero? é pontuação?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloadamos os recursos necessários:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.load('pt')\n",
    "parser = Portuguese()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E criamos a função "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokenized_text = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.is_digit:\n",
    "            continue\n",
    "        if token.is_space:\n",
    "            continue\n",
    "        if token.is_punct:\n",
    "            continue\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        #if token.is_title:\n",
    "        #    continue\n",
    "        if token.is_alpha:\n",
    "            tokenized_text.append(token.lower_)\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **limpar a lista com stopwords**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos a lista de `stopwords` que se encontrão no ficheiro `data/nlp/stopwords.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/nlp/stopwords.txt\") as file: \n",
    "    custom_stopwords = file.readlines()\n",
    "custom_stopwords = [line.strip() for line in custom_stopwords] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **stemmer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criamos um primeiro stemmer baseado [neste](https://www.nltk.org/_modules/nltk/stem/rslp.html) algoritmo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loadamos os dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     /Users/duarteocarmo/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('rslp')\n",
    "st = RSLPStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E criamos a função"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_rslp(tokens):\n",
    "    stemmed = []\n",
    "    for token in tokens:\n",
    "        stemmed.append(st.stem(word))\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **lemmatizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criamos um stemmer alternativo baseado num modelo de noticias da library `spacy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loadamos os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_pt = spacy.load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E criamos a função:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_spacy(tokens):\n",
    "    text = \" \".join(tokens)\n",
    "    lemma = []\n",
    "    for token in spacy_pt(text):\n",
    "        if token.lemma_:\n",
    "            lemma.append(token.lemma_)\n",
    "        else:\n",
    "            lemma.append(token)\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Função de limpeza de texto**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos finalmente prontos para criar uma função que recebe um discurso e limpa o totalmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = lemmatize_spacy(tokens)\n",
    "    #tokens = stem_rslp(tokens)\n",
    "    tokens = [token for token in tokens if token not in custom_stopwords]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos testar com o mesmo exemplo de acima:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto Original:\n",
      "Sr. Presidente, Srs. Membros do Governo, Sr.as Deputadas e Srs. Deputados: Dirijo um cumprimento especial aos estudantes de medicina que se encontram a assistir à sessão. Vou citar um ex-Primeiro-Ministro e começar por dizer o seguinte: quando queremos ser coerentes não podemos ser originais todos os dias. Este é um assunto de tal maneira debatido e replicado nesta Câmara que vou dizer exatamente aquilo que disse nas últimas três ou quatro vezes que este assunto foi aqui trazido repetidamente desde que os senhores são Governo e que a esquerda parlamentar os apoia. Vou dizê-lo em quatro notas particulares. Primeira nota: é evidentemente lesivo e um autêntico defraudar de expetativas, quer para os estudantes, quer para as suas famílias e, ainda pior, altamente lesivo para o Serviço Nacional de Saúde aquilo que é o desperdício de rios de dinheiro em formação médica sem depois haver uma consequência na formação médica especializada que, justamente, esses estudantes e esses jovens ambicionam e que legitimamente os utentes do SNS precisam.\n",
      "- - - - - - - - - - \n",
      "Texto Limpo:\n",
      "['presidente', 'sr', 'membro', 'governar', 'deputar', 'sr', 'deputar', 'dirigir', 'cumprimentar', 'especial', 'estudante', 'medicinar', 'encontrar', 'assistir', 'sessão', 'citar', 'começar', 'seguinte', 'querer', 'coerente', 'originar', 'assunto', 'maneiro', 'debater', 'replicar', 'n', 'câmara', 'exatamente', 'assunto', 'trazer', 'repetidamente', 'senhor', 'governar', 'esquerdo', 'parlamentar', 'noto', 'particular', 'noto', 'evidentemente', 'lesivo', 'autêntico', 'defraudar', 'expetativas', 'estudante', 'família', 'altamente', 'lesivo', 'serviço', 'nacional', 'saudar', 'desperdício', 'rio', 'dinheiro', 'formação', 'médico', 'haver', 'consequência', 'formação', 'médico', 'especializar', 'justamente', 'estudante', 'jovem', 'ambicionar', 'legitimamente', 'utente', 'sns', 'precisar']\n"
     ]
    }
   ],
   "source": [
    "texto_original = contents[indice]\n",
    "tokens_limpos = prepare_text(texto_original)\n",
    "texto_limpo = \" \".join(tokens_limpos)\n",
    "\n",
    "print(f\"Texto Original:\\n{texto_original}\")\n",
    "print(\"- \" * 10)\n",
    "print(f\"Texto Limpo:\\n{tokens_limpos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece produzir um resultado *aceitavel*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Detecção de tópicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Começamos por importatr algumas bibliotecas importantes como `gensim`. Com ela vamos usar um modelo `Latent Dirichlet allocation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim # lda e outros modelos\n",
    "import pickle # para guardar modelos\n",
    "from gensim import corpora # para criar um corpora instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Começamos por criar uma lista onde cada elemento é um lista que contém os \"tokens\" de um discurso de um deputado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 6988/6988 [03:58<00:00, 14.35it/s]\n"
     ]
    }
   ],
   "source": [
    "clean_text_data = []\n",
    "for i in tqdm(range(len(contents)), ascii=True):\n",
    "    if speakers[i] == \"Presidente (Jorge Lacão)\":\n",
    "        continue\n",
    "    else:\n",
    "        clean_text_data.append(prepare_text(contents[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De seguida, criamos dois elementos essenciais para a criação de um modelo LDA: \n",
    "- o dicionario de termos \n",
    "- e o corpus de \"bag of words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionnary = corpora.Dictionary(clean_text_data)\n",
    "corpus = [dictionnary.doc2bow(text) for text in clean_text_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos o corpus e o modelo no directorio `nlp_models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(corpus, open('nlp_models/corpus.pkl', 'wb'))\n",
    "dictionnary.save('nlp_models/dictionnary.gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma das desvantagens do modelo LDA é que temos de ser nós a definir o numero de topicos. Neste caso, escolhemos 20. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos agora criar e guardar o modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, \n",
    "                                          num_topics = NUM_TOPICS, \n",
    "                                          id2word=dictionnary, \n",
    "                                          passes=15)\n",
    "ldamodel.save('nlp_models/model5.gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos inspecionar os topicos de uma forma simples: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tópico #0 inclui: 0.054*\"social\" + 0.029*\"trabalhador\" + 0.024*\"coletiva\" + 0.022*\"segurança\" + 0.021*\"direito\" + 0.020*\"governar\" + 0.016*\"reformar\" + 0.016*\"contratação\" + 0.016*\"salário\"\n",
      "Tópico #1 inclui: 0.034*\"lembrar\" + 0.016*\"parecer\" + 0.015*\"negócio\" + 0.015*\"mutilação\" + 0.014*\"feminino\" + 0.013*\"comunicação\" + 0.013*\"mulher\" + 0.013*\"alves\" + 0.012*\"genital\"\n",
      "Tópico #2 inclui: 0.029*\"agricultura\" + 0.024*\"produção\" + 0.023*\"setor\" + 0.021*\"pequeno\" + 0.020*\"alimentar\" + 0.019*\"produto\" + 0.018*\"florestar\" + 0.014*\"rural\" + 0.014*\"região\"\n",
      "Tópico #3 inclui: 0.060*\"deputar\" + 0.051*\"concluir\" + 0.024*\"direito\" + 0.024*\"caducidade\" + 0.014*\"israel\" + 0.013*\"mulher\" + 0.013*\"armar\" + 0.011*\"liberdade\" + 0.011*\"militar\"\n",
      "Tópico #4 inclui: 0.035*\"governar\" + 0.017*\"ambientar\" + 0.015*\"verde\" + 0.015*\"população\" + 0.014*\"português\" + 0.012*\"deputar\" + 0.012*\"espanha\" + 0.011*\"d\" + 0.010*\"central\"\n",
      "Tópico #5 inclui: 0.018*\"escola\" + 0.016*\"público\" + 0.014*\"governar\" + 0.012*\"património\" + 0.011*\"d\" + 0.008*\"casar\" + 0.008*\"haver\" + 0.007*\"lisboa\" + 0.007*\"psd\"\n",
      "Tópico #6 inclui: 0.054*\"educação\" + 0.037*\"professorar\" + 0.026*\"ensinar\" + 0.026*\"escolar\" + 0.023*\"aluno\" + 0.022*\"base\" + 0.021*\"educativo\" + 0.020*\"escola\" + 0.016*\"pai\"\n",
      "Tópico #7 inclui: 0.018*\"público\" + 0.014*\"pessoa\" + 0.013*\"serviço\" + 0.011*\"garantir\" + 0.011*\"lei\" + 0.011*\"direito\" + 0.009*\"deficiência\" + 0.009*\"exatamente\" + 0.008*\"d\"\n",
      "Tópico #8 inclui: 0.096*\"deputar\" + 0.052*\"psd\" + 0.039*\"governar\" + 0.031*\"aplauso\" + 0.031*\"senhor\" + 0.027*\"terminar\" + 0.027*\"presidente\" + 0.025*\"cds\" + 0.023*\"protesto\"\n",
      "Tópico #9 inclui: 0.022*\"haver\" + 0.019*\"d\" + 0.019*\"deputar\" + 0.019*\"n\" + 0.012*\"lei\" + 0.012*\"proposto\" + 0.012*\"matéria\" + 0.009*\"governar\" + 0.009*\"problema\"\n",
      "Tópico #10 inclui: 0.032*\"ortográfico\" + 0.032*\"ensinar\" + 0.031*\"superior\" + 0.029*\"ciência\" + 0.024*\"camão\" + 0.024*\"língua\" + 0.021*\"científico\" + 0.017*\"instituição\" + 0.017*\"estudante\"\n",
      "Tópico #11 inclui: 0.030*\"milhão\" + 0.027*\"euro\" + 0.022*\"aumentar\" + 0.018*\"fiscal\" + 0.017*\"empresar\" + 0.016*\"banco\" + 0.015*\"orçamentar\" + 0.015*\"imposto\" + 0.015*\"pagar\"\n",
      "Tópico #12 inclui: 0.031*\"pcp\" + 0.029*\"votação\" + 0.029*\"aprovar\" + 0.028*\"lei\" + 0.028*\"be\" + 0.026*\"ps\" + 0.025*\"projeto\" + 0.025*\"resolução\" + 0.022*\"psd\"\n",
      "Tópico #13 inclui: 0.049*\"saudar\" + 0.019*\"serviço\" + 0.019*\"nacional\" + 0.011*\"cuidar\" + 0.011*\"deputar\" + 0.010*\"psd\" + 0.010*\"sr\" + 0.010*\"hospital\" + 0.010*\"médio\"\n",
      "Tópico #14 inclui: 0.172*\"deputar\" + 0.096*\"palavra\" + 0.047*\"parir\" + 0.039*\"pedir\" + 0.036*\"intervenção\" + 0.025*\"parlamentar\" + 0.023*\"responder\" + 0.022*\"grupar\" + 0.021*\"esclarecimento\"\n",
      "Tópico #15 inclui: 0.025*\"europeu\" + 0.016*\"político\" + 0.014*\"portugal\" + 0.013*\"união\" + 0.012*\"sr\" + 0.011*\"país\" + 0.010*\"portuguesar\" + 0.009*\"social\" + 0.008*\"europa\"\n",
      "Tópico #16 inclui: 0.027*\"animar\" + 0.013*\"água\" + 0.012*\"sociedade\" + 0.011*\"sensibilização\" + 0.011*\"rir\" + 0.011*\"pescador\" + 0.011*\"pan\" + 0.010*\"mau\" + 0.009*\"aprender\"\n",
      "Tópico #17 inclui: 0.038*\"presidente\" + 0.034*\"repúblico\" + 0.028*\"votar\" + 0.027*\"abril\" + 0.024*\"assembleia\" + 0.020*\"constituição\" + 0.019*\"sr\" + 0.017*\"parlamentar\" + 0.016*\"tribunal\"\n",
      "Tópico #18 inclui: 0.019*\"caçar\" + 0.013*\"linha\" + 0.013*\"medir\" + 0.011*\"orar\" + 0.010*\"comunicar\" + 0.010*\"alto\" + 0.010*\"acompanhamento\" + 0.008*\"sarampo\" + 0.008*\"zonar\"\n",
      "Tópico #19 inclui: 0.018*\"governar\" + 0.017*\"país\" + 0.015*\"portugal\" + 0.013*\"político\" + 0.012*\"d\" + 0.012*\"n\" + 0.011*\"aplauso\" + 0.011*\"haver\" + 0.011*\"psd\"\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.print_topics(num_words=9)\n",
    "for topic in topics:\n",
    "    print(f\"Tópico #{topic[0]} inclui: {topic[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (assembleia)",
   "language": "python",
   "name": "assembleia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
